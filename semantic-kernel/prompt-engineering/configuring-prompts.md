---
title: Configure prompts 
description: Learn how to configure prompts for your functions in Semantic Kernel
author: johnmaeda
ms.topic: skills
ms.author: johnmaeda
ms.date: 02/07/2023
ms.service: mssearch
---
# Configuring templates

[!INCLUDE [pat_large.md](../includes/pat_large.md)]

```File-Structure-For-Semantic-Skills
TestSkill
â”‚
â””â”€â”€â”€ SloganMaker
|    |
â”‚    â””â”€â”€â”€ skprompt.txt
â”‚    â””â”€â”€â”€ config.json
â”‚   
â””â”€â”€â”€ OtherFunction
     |
     â””â”€â”€â”€ skprompt.txt
     â””â”€â”€â”€ config.json
```

LLM AI [models](/semantic-kernel/concepts-ai/models) have a variety of parameters associated them that can alter their behavior. SK enables the developer to have complete control over how a model is to be configured by using a `config.json` file placed in the same directory as the `skprompt.txt` file.

```config.json-example
{
  "schema": 1,
  "type": "completion",
  "description": "a function that generates marketing slogans",
  "completion": {
    "max_tokens": 1000,
    "temperature": 0.0,
    "top_p": 0.0,
    "presence_penalty": 0.0,
    "frequency_penalty": 0.0
  }
}
```

The text used in `description` is arguably the most important parameter to consider because it's used by the [planner](/semantic-kernel/concepts-sk/planner) to get a quick read on what the function can do for a user.

> [!NOTE]
> The `config.json` file is currently optional, but if you wish to exercise precise control of a function's behavior be sure to include it inside each function directory. 

To learn more about the various parameters available for tuning how a function works, visit the [Azure OpenAI reference](/azure/cognitive-services/openai/reference).

## Default backends setting for OpenAI and Azure OpenAI

Learn more about [available GPT-3](/azure/cognitive-services/openai/concepts/models) models besides `text-davinci-003` for completion.

## Completion parameters that can be set in config.json

| Completion Parameter | Type | Required? | Default | Description |
|---|---|---|---|
| `max_tokens` | integer| Optional |16 |	The maximum number of tokens to generate in the completion. The token count of your prompt plus max_tokens can't exceed the model's context length. Most models have a context length of 2048 tokens (except davinci-codex, which supports 4096).|
| `temperature`	| number	| Optional	| 1	| What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. We generally recommend altering this or `top_p` but not both. |
| `top_p`	| number	| Optional	| 1	| An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both. |
| `presence_penalty` | number	| Optional	| 0	| Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. |
| `frequency_penalty` |	number	| Optional	|0 |	Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. |

## Take the next step

Running the app samples will give you the quickest sense of what you can do with SK. 

> [!div class="nextstepaction"]
> [Run the app samples](/semantic-kernel/samples)


---
title: LLM AI Tokens
description: Tokens
author: johnmaeda
ms.topic: concepts
ms.author: johnmaeda
ms.date: 02/07/2023
ms.service: mssearch
---
# What are Tokens?

[!INCLUDE [pat_large.md](../includes/pat_large.md)]

> [!TIP]
> Key topics: 
> * Tokens: basic units of text/code for LLM AI models to process/generate language.
> * Tokenization: splitting input/output texts into smaller units for LLM AI models.
> * Vocabulary size: the number of tokens each model uses, which varies among different GPT models.
> * Tokenization cost: affects the memory and computational resources that a model needs, which influences the cost and performance of running an OpenAI or Azure OpenAI model.
>
> _ðŸ‘†Topics list generated by semantic skill [`SummarizeSkill.Topics`](https://aka.ms/sk/repo/topics)_

_Tokens_ are the basic units of text or code that an LLM AI uses to process and generate language. Tokens can be characters, words, subwords, or other segments of text or code, depending on the chosen tokenization method or scheme. Tokens are assigned numerical values or identifiers, and are arranged in sequences or vectors, and are fed into or outputted from the model. Tokens are the building blocks of language for the model.
 
## How does tokenization work?

Tokenization is the process of splitting the input and output texts into smaller units that can be processed by the LLM AI models. Tokens can be words, characters, subwords, or symbols, depending on the type and the size of the model. Tokenization can help the model to handle different languages, vocabularies, and formats, and to reduce the computational and memory costs. Tokenization can also affect the quality and the diversity of the generated texts, by influencing the meaning and the context of the tokens. Tokenization can be done using different methods, such as rule-based, statistical, or neural, depending on the complexity and the variability of the texts.

OpenAI and Azure OpenAI uses a subword tokenization method called "Byte-Pair Encoding (BPE)" for its GPT-based models. BPE is a method that merges the most frequently occurring pairs of characters or bytes into a single token, until a certain number of tokens or a vocabulary size is reached. BPE can help the model to handle rare or unseen words, and to create more compact and consistent representations of the texts. BPE can also allow the model to generate new words or tokens, by combining existing ones. The way that tokenization is different dependent upon the different model Ada, Babbage, Curie, and Davinci is mainly based on the number of tokens or the vocabulary size that each model uses. Ada has the smallest vocabulary size, with 50,000 tokens, and Davinci has the largest vocabulary size, with 60,000 tokens. Babbage and Curie have the same vocabulary size, with 57,000 tokens. The larger the vocabulary size, the more diverse and expressive the texts that the model can generate. However, the larger the vocabulary size, the more memory and computational resources that the model requires. Therefore, the choice of the vocabulary size depends on the trade-off between the quality and the efficiency of the model.

## What does tokenization have to do with the cost of running a model?

Tokenization affects the amount of data and the number of calculations that the model needs to process. The more tokens that the model has to deal with, the more memory and computational resources that the model consumes. Therefore, the cost of running an OpenAI or Azure OpenAI model depends on the tokenization method and the vocabulary size that the model uses, as well as the length and the complexity of the input and output texts. Based on the number of tokens used for interacting with a model and the different rates for different models, your costs can widely differ. For example, as of February 2023, the rate for using Davinci is $0.06 per 1,000 tokens, while the rate for using Ada is $0.0008 per 1,000 tokens. The rate also varies depending on the type of usage, such as playground, search, or engine. Therefore, tokenization is an important factor that influences the cost and the performance of running an OpenAI or [Azure OpenAI model](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/).

## Take the next step

> [!div class="nextstepaction"]
> [Learn about embeddings](/semantic-kernel/concepts-ai/embeddings)

[!INCLUDE [footer.md](../includes/footer.md)]